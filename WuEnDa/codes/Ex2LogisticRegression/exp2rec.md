# 实验二、Logistic Regession 分类问题
### 实验大纲
在本次实验中, 你需要建立一个 逻辑回归模型(logistic regression model, 对率回归模型) 来预测一名学生是否可以倍录取进入大学。假定你是某高校的管理者，你想要依据两门考试的分数来决定一位受试者被录取的几率，现在你手上有一些历史数据，他们给出了每位受试者两门考试的成绩以及录取情况，请据此建立一个 逻辑回归模型 来帮助你作出决策。

### 实验步骤


### 实验记录
* 遍历DataFrame的每一行(事实上, 我们应当总是将数据按列处理, 也即最好是以向量的形式处理)
  问题在于将点集绘制出来, 是否被录取应当使用不同的颜色or标记, 此时我想到的易于实现的方法是遍历每一个数据
  ```python
  for index, row in df.iterrows():
    print row["c1"], row["c2"]
  ```
* 遇到了一个很严重的问题, 这里可以说涉及到 `Logistic Regression Model` 的立身之根本。
  * 依据对线性回归模型的理解，定义一个模型最核心的是 1.定义假设函数 2. 定义 代价函数costFunc 以测量样本与假设之间的 距离 2. 定义 梯度 以确定参数改变的方向, 通常需要求对参数的偏导, 这就对 costFunc 提出了要求
  * 在 线性回归模型中, 我们的假设函数是 特征的线性组$X\theta^T$, 距离为 均方差, 描述整个数据集与假设函数间的距离, 这一定义十分自然美观, 可以对$\theta_j$求偏导, 并作为$\theta_j$的改变方向
  * 针对分类问题, 比如二分类, 此时样本标记不再是连续值而是$y\in{\{0, 1\}}$, 设想我们仍然使用线性回归用一条直线去拟合这些点, 那么再设定一个 阈值, 例如0.5, 那么对应的就可以划分了, 问题在于$h(x;\theta)$值域不一定在(0,1)之间, 因此需要一个函数变换使$X\theta^T$位于(0,1), 直接给出为 $g(z)=\frac{1}{1+e^z}$, 他是一类sigmoid函数(S型曲线), 图形夹在y=0和y=1之间. 此时$h(x;\theta)=g(X\theta^T)$. 注意没有使用这样的策略, 也即类似于符号函数的定义, 这是由于后面能够求解梯度. 此外, 其距离不再是均方误差, 而是使用对数表示距离: $cost = -y*ln(h)-(1-y)*ln(1-h)$, 若y=1, cost = -ln(h), h越接近0, cost越大. 据此, 得到的梯度下降算法形式, 竟与线性回归无二. 因而, 可以使用同样的向量化方法使用一模一样的步骤求解问题
  * 原始数据data插入一列1, 划分X, y, 初始化$\theta$为0, 写出sigmoid函数, 写出costFunc函数, 写出gradientDescent, 初始化迭代次数iter_n和学习率alpha.进行梯度下降
  * 接下来百思不得其解的问题发生了, 首先是cost计算均为Nan非数, 这一点经过检查发现是$\Delta_j$的计算中忘记除以m了, 排除后, 计算得到发现sot一直在震荡, 没有无限增长, 也没有收敛. 此时我猜测是学习率过大的原因?于是不断调小学习率, 从0.3开始一直调, 期间仍是震荡, 直到$\alpha=0.0002$左右, 忽然发现cost收敛了！于是迅速画出 决策界限 与散点图, 发现两者是天差地别, 十分失望, 又检查了costFunc gradientDescnt等函数并多次调节$alpha$作图, 结果是cost收敛于一个较大的值(7), 然后我忽然想到, $\theta$就一定是0吗, 是否还与$\theta$初值选定有关呢, 我立即填入几个数, 实际上我心里大概知道, 这些参数确定的界限其大致走向是不会错的, 果然, 此时决策曲线要好得多, 但是还是差得很远. 一开始我没去管常数项$\theta_0$, 设定为y均值, 反复调节其它两个参数, 发现cost确实能够收敛, 比之前的小很多, 但曲线似乎没怎么变, 仍是差的很远, 于是我又尝试着把调好的两个参数固定, 又去调节所谓常数项, 发现把他越往小调节cost越小, 直到变成$\theta_0$又变回0, cost很小到0.6了, 但此时曲线又不稳定了, 一下子又相差十万八千里, 给我一种cost大大减少但曲线并未贴合的感受, 我突然又想到, 有没有可能$\theta_0$是负数, 于是又调了两下, 很好, 界限划分的很好, 此时cost大约仅有0.2左右
  * 因此, 仅仅是这些参数的初值选取, 就会让结果出现各种意想不到的情况, 一度以为理论和实际脱钩, 因此, 可以窥见, 参数选取真的会十分磨人, 并且还是建立在人的经验知识之上, 至少, 在这种直接的梯度算法 没有辅以各种更深入的算法里 是这样

* [fmin_tnc](https://www.cnblogs.com/tongtong123/p/10634716.html)
  有约束的多元函数问题，提供梯度信息，使用截断牛顿法。
  func：优化的目标函数； x0：初值； fprime：提供优化函数func的梯度函数，不然优化函数func必须返回函数值和梯度，或者设置approx_grad=True
  approx_grad :如果设置为True，会给出近似梯度
  args：元组，是传递给优化函数的参数
  返回：
  x ： 数组，返回的优化问题目标值
  nfeval ： 整数，function evaluations的数目
  在进行优化的时候，每当目标优化函数被调用一次，就算一个function evaluation。在一次迭代过程中会有多次function evaluation。这个参数不等同于迭代次数，而往往大于迭代次数。

* 9-25 Sunday
  1. 尝试一个二次函数的拟合
  2. 正则化的理论推断（先不写代码）
  3. 西瓜书第4章
  4. 视频第8章 ---- 一些非线性的假设（正式进入网络）