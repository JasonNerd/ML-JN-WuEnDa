 # 2、无监督学习算法(Unsupervised Learning Algorithm)
 这是一个激动人心的时刻，此前，我们学习的都是监督学习的算法，他们处理的数据都是带标签的数据，例如处理带连续值标签的线性回归算法以及用于典型的离散值标签分类问题的对率回归、神经网络以及支持向量机算法等等。而在无监督学习的任务中，我们拿到的将是一组没有任何标签的数据。
 $$Training set: {x^{(1)}, x^{(2)}, ..., x^{(m)}}$$
 ---
## 2-1、聚类分析(Clustering)
### 2-1（1）引言
聚类分析算法是一类典型的无监督学习算法，他试图从一系列的无标签训练数据中发现数据的内在结构（聚簇划分）。如下图，我们可以让聚类算法帮助我们画出两个圈，将数据集分为两个簇(cluster)
![](http://www.ai-start.com/ml2014/images/6709f5ca3cd2240d4e95dcc3d3e808d5.png)

### 2-1（2）应用场景举例
***市场划分(Market Segementation)***：也许你在数据库中存储了许多客户的信息，而你希望将他们分成不同的客户群，这样你可以对不同类型的客户分别销售产品或者分别提供更适合的服务。
***社交网络分析(Social network analysis)***：简单来说就是依据人的社交活动划出一个个的社交圈，比如说：你经常跟哪些人联系，而这些人又经常给哪些人发邮件，由此找到关系密切的人群。
***计算机集群管理(Organize computing clusters)***：希望使用聚类算法来更好的组织计算机集群，或者更好的管理数据中心，有哪些计算机经常协作工作。
***星系形成(Astronomical data analysis)***：最后，我实际上还在研究如何利用聚类算法了解星系的形成。然后用这个知识，了解一些天文学上的细节问题。

### 2-1（3）K-均值算法的步骤
&emsp;&emsp;K-均值是最普及的聚类算法，算法接受一个未标记的数据集，然后将数据聚类成不同的组。
K-均值算法是一个迭代算法
第一步，在样本空间中随机选取K个点，称之为 ***聚类中心(cluster centroids)*** ;

第二步，对于数据集$\{x^{(i)}_{i=1...m}\}$中的每一个样本点，计算样本点与K个样本中心的距离$\{d^{(i)}_1, d^{(i)}_2, ..., d^{(i)}_K\}$，选取其中最小的距离$d^{(i)}_k$，将点$x^{(i)}$与聚类中心$k$关联起来，也即属于第k类，这一步为聚类。

第三步，对于第二步形成的K个聚类，得到新的聚类中心为该类所有样本点求均值得到的点

重复第二步和第三步直到新的聚类中心与原聚类中心基本重合。

### 2-1（4）k-均值算法的优化目标
由第3小节关于K-均值算法步骤的分析可知，算法的优化目标是样本点与所属聚类的聚类中心的距离的和的均值。代价函数（又称 ***畸变函数distortion function*** ）。
![](https://files.mdnice.com/user/35698/ca374566-b50f-41bb-9ffe-303a2bc4fb1d.png)

### 2-1（5）k-均值算法的细节问题
**如何得到随机K个聚类中心**
方法是随机选取K个训练示例作为聚类中心，其次K-均值的一个问题在于，它有可能会停留在一个局部最小值处，而这取决于初始化的情况。方法是多次运行K-均值算法，每一次都重新进行随机初始化，最后再比较多次运行K-均值的结果，选择代价函数最小的结果。这种方法在较小的时候（2--10）还是可行的，但是如果K较大，这么做也可能不会有明显地改善。
![](http://www.ai-start.com/ml2014/images/d4d2c3edbdd8915f4e9d254d2a47d9c7.png)

**如何选取K的值**
一种考量是“肘部法则elbow rule”:
![](http://www.ai-start.com/ml2014/images/f3ddc6d751cab7aba7a6f8f44794e975.png)

但通常更常见的是基于实际需要的考量
例如，我们的 T-恤制造例子中，我们要将用户按照身材聚类，我们可以分成3个尺寸S M L:，也可以分成5个尺寸XS S M L XL，这样的选择是建立在回答“聚类后我们制造的T-恤是否能较好地适合我们的客户”这个问题的基础上作出的。


## 2-2、降维(Dimensionality Reduction)
### 2-2（1）降维的动机
首先，让我们谈论降维是什么。作为一种生动的例子，我们收集的数据集，有许多特征，我绘制两个在这里。假设我们未知两个的特征: $x_1$是用厘米表示的高度；$x_2$是用英寸表示同一物体的高度。所以，这给了我们高度冗余表示：
![](http://www.ai-start.com/ml2014/images/8274f0c29314742e9b4f15071ea7624a.png)
因而，我们希望将这个二维的数据降至一维。这样的处理过程可以被用于把任何维度的数据降到任何想要的维度，例如将1000维的特征降至100维。
![](http://www.ai-start.com/ml2014/images/67e2a9d760300d33ac5e12ad2bd5523c.jpg)
一方面，对数据进行降维可以压缩数据量，这被称为数据压缩(data compressing)，它使得可以占用较少的计算机内存或磁盘空间，但它也让我们加快我们的学习算法。另一方面，由于维度的减小，数据可视化也成为了可能。
![](http://www.ai-start.com/ml2014/images/789d90327121d3391735087b9276db2a.png)
假使我们有有关于许多不同国家的数据，每一个特征向量都有50个特征（如GDP，人均GDP，平均寿命等）。如果要将这个50维的数据可视化是不可能的。使用降维的方法将其降至2维，我们便可以将其可视化了。
![](http://www.ai-start.com/ml2014/images/ec85b79482c868eddc06ba075465fbcf.png)

### 2-2（2）降维算法--PCA(Principal Component Analysis)
主成分分析(PCA)是最常见的降维算法
在PCA中，我们要做的是***找到一个方向向量（Vector direction），当我们把所有的数据都投射到该向量上时，我们希望投射平均均方误差能尽可能地小***。方向向量是一个经过原点的向量，而投射误差是从特征向量向该方向向量作垂线的长度。

PCA问题的描述：将$n$维数据点降到$k$维，目标是找到$k$维空间的一组基$u^{(1)}, u^{(2)}, ..., u^{(k)}$使得总体投影最小。

PCA算法步骤：
第一步是**均值归一化**。我们需要计算出所有特征的均值$\mu_i$，然后作差$x_i-\mu_i$ 。如果特征是在不同的数量级上，我们还需要将其除以标准差$\delta$

第二步计算**协方差矩阵(covariance matrix)$\Sigma$:
$$\Sigma = \frac{1}{m}\sum_{i=1}^m[x^{(i)}{x^{(i)}}^T]$$
每一个样本点$x^{(i)}$都是一个$n$维的列向量

第三步是计算协方差矩阵$\Sigma$的**特征向量(eigenvectors)**:
编程时可使用奇异值分解(Singular Value Decomposition)求解`[U, S, V] = svd(Sigma)`
![](http://www.ai-start.com/ml2014/images/0918b38594709705723ed34bb74928ba.png)

最后取矩阵$U$的前$k$个列向量得到$U_{reduce}$，他是$n\times k$维的矩阵，于是对于每一个样本点$x^{(i)}$，计算$z^{(i)} = U^T_{reduce}x^{(i)}$，它是$k$维的向量，此即为样本点投影到新的$k$维空间的坐标。

### 2-2（3）PCA算法的细节问题
#### 如何选择主成分数量k

训练集方差: $Var_{train}=\frac{1}{m}\sum^m_{i=1}||x^{(i)}||^2$
投影误差: $Var_{proj}=\frac{1}{m}\sum^m_{i=1}||x^{(i)}-U_{reduce}z^{(i)}||^2$
则相对误差定义为: $\alpha = \frac{Var_{proj}}{Var_{train}}$

回忆SVD分解中有: `[U, S, V] = svd(sigma)`。其中$S$是一个对角矩阵，也即$S$除主对角线外其他元素均为0。因而一个更简单的解法是：
$\alpha = 1-S_k/S_n$。其中$S_i$表示$S$主对角线前$i$个元素之和。

#### 主成分分析法的应用建议

以下场合不建议使用PCA算法 **非必要不使用**

一个常见错误使用主要成分分析的情况是，将其用于减少过拟合（减少了特征的数量）。这样做非常不好，不如尝试正则化处理。原因在于主要成分分析只是近似地丢弃掉一些特征，它并不考虑任何与结果变量有关的信息，因此可能会丢失非常重要的特征。

另一个常见的错误是，默认地将主要成分分析作为学习过程中的一部分，这虽然很多时候有效果，最好还是从所有原始特征开始，只在有必要的时候（算法运行太慢或者占用太多内存）才考虑采用主要成分分析。
