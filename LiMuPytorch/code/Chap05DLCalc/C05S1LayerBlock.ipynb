{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第1节-层(Layer)和块(block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现一个MLP块, 它包含一个节点数量为num_hidden的隐含层, 接受特征数量为num_in的输入, 经过线性变换+激活函数+线性变换得到分量数目大小为num_out的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "class MyMLP(nn.Module):\n",
    "    def __init__(self, num_in, num_hidden, num_out):\n",
    "        super().__init__()\n",
    "        self.linearI = nn.Linear(num_in, num_hidden)\n",
    "        self.linearO = nn.Linear(num_hidden, num_out)\n",
    "    def forward(self, X):\n",
    "        return self.linearO(F.relu(self.linearI(X)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2., 3., 4., 5.],\n",
       "         [5., 3., 2., 1.]]),\n",
       " torch.Size([2, 4]),\n",
       " tensor([[-1.1490,  2.7165],\n",
       "         [-0.7219,  0.3292]], grad_fn=<AddmmBackward0>),\n",
       " torch.Size([2, 2]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp1 = MyMLP(4, 32, 2)\n",
    "iX = torch.tensor([[2, 3, 4, 5], [5, 3, 2, 1.0]])\n",
    "oy = mlp1(iX)\n",
    "iX, iX.shape, oy, oy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意在以上MyMLP的实现中有一些细节, 一是初始化调用父类默认初始化, 这里可以是可以自定义的, 另一个是MyMLP仅重写了foward方法, 在实例化MyMLP得到一个对象mlp1后, mlp1(iX)也即完成了forward方法调用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "实现一个MySequential类, 它应当具有与Sequential完全相同的行为, 也即对于传入的模块, 应该按照顺序像链条一样的执行它们"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySeqList(nn.Module):\n",
    "    def __init__(self, *modules):\n",
    "        super().__init__()\n",
    "        self.modules = [] # 使用列表存放了各个模型\n",
    "        for m in modules:\n",
    "            self.modules.append(m)\n",
    "    def forward(self, X):\n",
    "        o = X\n",
    "        for m in self.modules:\n",
    "            o = m(o)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(MySequential(),\n",
       " tensor([[ 0.2682, -0.7114],\n",
       "         [ 0.6078, -0.3268]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp2 = MySeqList(nn.Linear(4, 32), nn.ReLU(), nn.Linear(32, 2))\n",
    "oy2 = mlp2(iX)\n",
    "mlp2, oy2 # 似乎没什么问题?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`for i, val in enumrate(args)`意思是枚举所有的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(MySequential(\n",
       "   (0): Linear(in_features=4, out_features=32, bias=True)\n",
       "   (1): ReLU()\n",
       "   (2): Linear(in_features=32, out_features=2, bias=True)\n",
       " ),\n",
       " tensor([[0.2972, 0.8175],\n",
       "         [0.3226, 0.7153]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for i, m in enumerate(args):\n",
    "            self._modules[str(i)] = m # 顺序枚举, 存入OrderedDict\n",
    "    def forward(self, X):\n",
    "        for m in self._modules.values():\n",
    "            X = m(X)\n",
    "        return X\n",
    "mlp3 = MySequential(nn.Linear(4, 32), nn.ReLU(), nn.Linear(32, 2))\n",
    "oy3 = mlp3(iX)\n",
    "mlp3, oy3 \n",
    "# 这里就体现出区别了, 内部细节是可以打印出来的, 问题的关键在于顺序的数据结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 把模块嵌套起来\n",
    "接下来, 试着实现一个嵌套的模块吧, 它可以由您任意的定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n",
      "DoubleEleven(\n",
      "  (l1): MyMLP(\n",
      "    (linearI): Linear(in_features=3, out_features=32, bias=True)\n",
      "    (linearO): Linear(in_features=32, out_features=12, bias=True)\n",
      "  )\n",
      "  (l2): MyMLP(\n",
      "    (linearI): Linear(in_features=12, out_features=16, bias=True)\n",
      "    (linearO): Linear(in_features=16, out_features=32, bias=True)\n",
      "  )\n",
      "  (l3): MyMLP(\n",
      "    (linearI): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (linearO): Linear(in_features=16, out_features=8, bias=True)\n",
      "  )\n",
      "  (net): MySequential(\n",
      "    (0): MyMLP(\n",
      "      (linearI): Linear(in_features=3, out_features=32, bias=True)\n",
      "      (linearO): Linear(in_features=32, out_features=12, bias=True)\n",
      "    )\n",
      "    (1): ReLU()\n",
      "    (2): MyMLP(\n",
      "      (linearI): Linear(in_features=12, out_features=16, bias=True)\n",
      "      (linearO): Linear(in_features=16, out_features=32, bias=True)\n",
      "    )\n",
      "    (3): ReLU()\n",
      "    (4): MyMLP(\n",
      "      (linearI): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (linearO): Linear(in_features=16, out_features=8, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "tensor([[-0.0091, -0.0433, -0.0176,  0.0891,  0.2928,  0.2777, -0.2357,  0.1020],\n",
      "        [-0.0152, -0.0463, -0.0182,  0.0855,  0.2912,  0.2775, -0.2361,  0.0970],\n",
      "        [-0.0182, -0.0440, -0.0197,  0.0831,  0.2918,  0.2787, -0.2376,  0.0969],\n",
      "        [-0.0047, -0.0464, -0.0157,  0.0954,  0.2953,  0.2800, -0.2336,  0.1026]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 这是一个 名为 【双十一】 的神经网络模块哦，今天啊，11月3号，双十一第一轮结束了呢\n",
    "class DoubleEleven(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = MyMLP(3, 32, 12)\n",
    "        self.l2 = MyMLP(12, 16, 32)\n",
    "        self.l3 = MyMLP(32, 16, 8)\n",
    "        self.net =  net = MySequential(self.l1, nn.ReLU(), self.l2, nn.ReLU(), self.l3)\n",
    "    def forward(self, X):\n",
    "        return self.net(X)\n",
    "\n",
    "m1 = DoubleEleven()\n",
    "iX = torch.tensor([[1.0, 2.2, 3.2], [2, 1, 3], [3, 1, 2], [2, 3, 1]])\n",
    "print(iX.shape)\n",
    "print(m1)\n",
    "print(m1(iX)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来的话, 想实现这样一个模块，这一个模块接受两个模块net1, net2作为参数, 把他们的输出串起来作为输出，这称为并行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n",
      "tensor([[-0.1334, -0.2063, -0.1213, -0.1381, -0.0088,  0.2753, -0.1507, -0.3019,\n",
      "         -0.7577,  0.1774, -0.4720, -0.0535, -0.4564, -0.1063,  0.7908],\n",
      "        [-0.1334, -0.2066, -0.1204, -0.1366, -0.0088,  0.2740, -0.1529, -0.3000,\n",
      "         -0.5569,  0.0771, -0.4934, -0.1579, -0.2295,  0.1907,  0.4243],\n",
      "        [-0.1299, -0.2072, -0.1208, -0.1367,  0.0011,  0.2685, -0.1549, -0.3033,\n",
      "         -0.3735, -0.0166, -0.4615, -0.1571, -0.1250,  0.2659,  0.3180],\n",
      "        [-0.1307, -0.2048, -0.1196, -0.1382,  0.0010,  0.2663, -0.1555, -0.2996,\n",
      "         -0.4522,  0.0484, -0.6037,  0.3139, -0.3515,  0.0557,  0.6013]],\n",
      "       grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class Parallel(nn.Module):\n",
    "    def __init__(self, net1, net2):\n",
    "        super().__init__()\n",
    "        self.net1 = net1\n",
    "        self.net2 = net2\n",
    "    def forward(self, X):\n",
    "        o1 = self.net1(X)\n",
    "        o2 = self.net2(X)\n",
    "        return torch.concat([o1, o2], dim=1)\n",
    "\n",
    "m1 = DoubleEleven() # 3 --> 8\n",
    "m2 = MyMLP(3, 15, 7) # 3 -->7\n",
    "pm = Parallel(m1, m2)\n",
    "print(iX.shape)\n",
    "opm = pm(iX)\n",
    "print(opm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['net1.l1.linearI.weight', 'net1.l1.linearI.bias', 'net1.l1.linearO.weight', 'net1.l1.linearO.bias', 'net1.l2.linearI.weight', 'net1.l2.linearI.bias', 'net1.l2.linearO.weight', 'net1.l2.linearO.bias', 'net1.l3.linearI.weight', 'net1.l3.linearI.bias', 'net1.l3.linearO.weight', 'net1.l3.linearO.bias', 'net1.net.0.linearI.weight', 'net1.net.0.linearI.bias', 'net1.net.0.linearO.weight', 'net1.net.0.linearO.bias', 'net1.net.2.linearI.weight', 'net1.net.2.linearI.bias', 'net1.net.2.linearO.weight', 'net1.net.2.linearO.bias', 'net1.net.4.linearI.weight', 'net1.net.4.linearI.bias', 'net1.net.4.linearO.weight', 'net1.net.4.linearO.bias', 'net2.linearI.weight', 'net2.linearI.bias', 'net2.linearO.weight', 'net2.linearO.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(pm.state_dict().keys()) # OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "就目前而言，我们所做的事就是把已经定义好的模块进行组合嵌套，在forward上也是如此，使用默认的输出方式，那么forward是否可以包含一般python计算程序中的控制流呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 2.2000, 3.2000],\n",
      "        [2.0000, 1.0000, 3.0000],\n",
      "        [3.0000, 1.0000, 2.0000],\n",
      "        [2.0000, 3.0000, 1.0000]])\n",
      "Verbose(\n",
      "  (linear): Linear(in_features=3, out_features=5, bias=True)\n",
      ")\n",
      "tensor(0.6649, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class Verbose(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 5)\n",
    "        # 随机初始化的constant权重, 不参与反向传播\n",
    "        self.rand_constant_weight = torch.randn(size=(5, 4), requires_grad=False)\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        X = F.relu(torch.matmul(X, self.rand_constant_weight)+1)\n",
    "        while torch.abs(X).sum() > 1:\n",
    "            X = X/2\n",
    "        return X.sum()\n",
    "\n",
    "vm = Verbose()\n",
    "print(iX)\n",
    "print(vm)\n",
    "print(vm(iX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('linear.weight', tensor([[-0.4789, -0.4629, -0.3664],\n",
      "        [ 0.2398, -0.4736, -0.5287],\n",
      "        [-0.2510,  0.0498,  0.4063],\n",
      "        [ 0.2895,  0.2333,  0.5280],\n",
      "        [ 0.5192, -0.2114,  0.3307]])), ('linear.bias', tensor([ 0.4119,  0.3542, -0.5392, -0.3425, -0.4594]))])\n",
      "tensor([[ 0.2239, -0.2493,  1.2607, -2.0692],\n",
      "        [-0.3456,  0.6365,  1.1249, -0.7855],\n",
      "        [ 0.4827,  0.0264, -1.0951,  0.2563],\n",
      "        [ 0.8257, -0.1934,  1.4364,  0.5306],\n",
      "        [-0.5577,  0.7912, -0.9933, -0.0128]])\n"
     ]
    }
   ],
   "source": [
    "print(vm.state_dict())\n",
    "print(vm.rand_constant_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[python函数之传递多个参数](https://blog.csdn.net/u011607898/article/details/107585700)\n",
    "1、在python自定义函数中，如果需要传入的实际参数有多个，我们在定义形式参数的时候，可以有两种形式，一是`*parameter`，二是`**parameter`。这两种分别提供了传入的参数是多个的形式。\n",
    "`*parameter`表示接收任意多个实际参数并将其放到一个元组中，类似于传递地址的形式，将多个数据一次性传入。\n",
    "```\n",
    "def printcoff(*para):\n",
    "\tfor item in para:\n",
    "\t\tprint(item)\n",
    "printcoff(\"karl\",\"inter\",\"killer\")\n",
    "plist = [1,2,3,4,5,6,7,8,9,0]\n",
    "printcoff(*plist)\n",
    "```\n",
    "`**parameter`表示接受任意多个类似关键字参数一样显示赋值的实际参数，并将其放到一个字典中。\n",
    "```\n",
    "def printcoff(**para):\n",
    "\tfor key, value  in para.items():\n",
    "\t\tprint(key,value)\n",
    "pdict = {\"1\":\"karl\",\"2\":\"inter\",\"3\":\"killer\",\"4\":\"python\"}\n",
    "printcoff(**pdict)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "01a1107180bc694675be9004b07acea2b4135fa6d978556db60a7effbd2129f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
