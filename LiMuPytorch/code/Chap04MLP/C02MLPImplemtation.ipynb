{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd89966b",
   "metadata": {},
   "source": [
    "# 单隐含层感知机网络的实现\n",
    "1. 数据集： FashionMinist数据集, 训练集60000条数据, 验证集1000条, 输入特征维数$28*28=784$，输出分类数为10, 以batch_size=256的大小读取数据集。\n",
    "<br>\n",
    "2. 感知机网络结构：隐含层256个节点，两个权重矩阵W1和W2, W1连接了输入层和隐含层，形状是(d, h) = (784, 256), W2连接的是隐含层和输出层，形状是(h, q) = (256, 10)。两个偏置列向量b1和b2，维度分别是h=256以及q=10。另外隐含层节点的非线性变换是ReLU函数。输入数据是维度是(n, d)=(256, 784)，输出是(n, q) = (256, 10)\n",
    "<br>\n",
    "3. 交叉熵损失和优化函数SGD：进行参数训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbe2be4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "# 1. 导入数据\n",
    "batch_size = 256\n",
    "trans = transforms.ToTensor()\n",
    "minist_train = FashionMNIST(root=\"../data\", train=True, transform=trans)\n",
    "minist_test = FashionMNIST(root=\"../data\", train=False, transform=trans)\n",
    "train_iter = DataLoader(minist_train, shuffle=True, num_workers=4, batch_size=256)\n",
    "test_iter = DataLoader(minist_test, shuffle=False, num_workers=4, batch_size=256)\n",
    "for X, y in train_iter:\n",
    "    print(X.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ded9b5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 构造感知器网络\n",
    "from torch import nn\n",
    "input_size = 28*28\n",
    "output_size = 10\n",
    "hidden_size = 256\n",
    "# W1(input_size, hidden_size)  W2(hidden_size, output_size)\n",
    "# b1(hidden_size, 1) b2(output_size, 1)\n",
    "# X(batch_size, input_size) y_hat(batch_size, output_size)\n",
    "W1 = nn.Parameter(torch.normal(0, 0.1, size=(input_size, hidden_size), requires_grad=True))\n",
    "W2 = nn.Parameter(torch.normal(0, 0.1, size=(hidden_size, output_size), requires_grad=True))\n",
    "b1 = nn.Parameter(torch.zeros(hidden_size, requires_grad=True))\n",
    "b2 = nn.Parameter(torch.zeros(output_size, requires_grad=True))\n",
    "\n",
    "def relu(X):\n",
    "    return torch.max(X, torch.zeros_like(X))\n",
    "\n",
    "# 这里“@”代表矩阵乘法\n",
    "def net(X):\n",
    "    Hz = torch.matmul(X, W1) + b1 # 第一层的仿射变换\n",
    "    return torch.matmul(relu(Hz), W2) + b2 # 套上ReLU再进行第二层仿射变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "387ff9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: train-loss = 0.84860\n",
      "epoch 2: train-loss = 0.58720\n",
      "epoch 3: train-loss = 0.53083\n",
      "epoch 4: train-loss = 0.49957\n",
      "epoch 5: train-loss = 0.47825\n",
      "epoch 6: train-loss = 0.46328\n",
      "epoch 7: train-loss = 0.45256\n",
      "epoch 8: train-loss = 0.44205\n",
      "epoch 9: train-loss = 0.43252\n",
      "epoch 10: train-loss = 0.42468\n"
     ]
    }
   ],
   "source": [
    "# 3. 损失函数以及优化函数\n",
    "lr = 0.03\n",
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "sgd = torch.optim.SGD(params=[W1, W2, b1, b2], lr=lr)\n",
    "# 4. 进行训练\n",
    "def train(train_iter, epoch_num, net, loss, optim):\n",
    "    for epoch in range(epoch_num):\n",
    "        tl, en = 0, 0\n",
    "        for X, y in train_iter:\n",
    "            y_hat = net(X.reshape(-1, input_size))\n",
    "            l = loss(y_hat, y)\n",
    "            optim.zero_grad()\n",
    "            l.backward()\n",
    "            optim.step()\n",
    "            en += len(y)\n",
    "            tl += len(y)*l\n",
    "        with torch.no_grad():\n",
    "            print(f\"epoch {epoch+1}: train-loss = {tl/en:.5f}\")\n",
    "\n",
    "train(train_iter, 10, net, cross_entropy, sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a042052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: acc=51163, total=60000, accuracy=85.272%\n",
      "Testing set: acc=8393, total=10000, accuracy=83.930%\n"
     ]
    }
   ],
   "source": [
    "# 计算训练集和验证集的准确度\n",
    "def evaluate(data_iter, net):\n",
    "    acc, tt = 0, 0\n",
    "    for X, y in data_iter:\n",
    "        with torch.no_grad():\n",
    "            y_hat = net(X.reshape(-1, input_size))\n",
    "            y_pre = torch.argmax(y_hat, dim=1)\n",
    "            acc += (y_pre==y).sum()\n",
    "            tt += len(y)\n",
    "    return acc, tt\n",
    "\n",
    "acc, tt = evaluate(train_iter, net)\n",
    "print(f\"Training set: acc={acc}, total={tt}, accuracy={(acc/tt)*100:.3f}%\")\n",
    "acc, tt = evaluate(test_iter, net)\n",
    "print(f\"Testing set: acc={acc}, total={tt}, accuracy={(acc/tt)*100:.3f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71992fc",
   "metadata": {},
   "source": [
    "1. `torch.zeros_like(X)`: 创建一个形状和X相同的0张量\n",
    "2. `torch.max(S, V)`: 是逐元素操作的函数，S和V形状相同\n",
    "```py\n",
    "tst_aa = torch.tensor([[1, -2.0], [2, -1.0]])\n",
    "relu(tst_aa)\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58bb4bd",
   "metadata": {},
   "source": [
    "广播机制\n",
    "```py\n",
    "a = torch.tensor([[1, -2.0], [2, -1.0], [-1.0, -2.0]])\n",
    "b = torch.tensor([[1], [2], [3]])\n",
    "a + b # ok, a is (3, 2) and b is (3, 1) --> 按列广播\n",
    "b = torch.tensor([[1, 2]])\n",
    "a + b # ok, now b is (1, 2), we can transpose b by row\n",
    "b = torch.tensor([1, 2])\n",
    "a + b \n",
    "# of course right, now b is a vector, just add b to each row of a\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1129b474",
   "metadata": {},
   "source": [
    "`loss = nn.CrossEntropyLoss(reduction='mean')`\n",
    "> reduction: （string，可选）”none”：不应用任何缩减，“mean”：取输出的加权平均值，“sum”：输出将被求和。\n",
    "\n",
    "定义loss后，通常这样使用 l = loss(y_hat, y), 其中y_hat和y是相同维度的向量, 顺序不可颠倒\n",
    "交叉熵损失的内部实现其实包含了softmax操作，也即对于y_hat的每一行进行了e指数归一化，随后是熵操作，也即取负对数，随后按照每一个y的分量进行index后求和或取平均或者直接返回向量形式\n",
    "\n",
    "参考:\n",
    "【1】[nn.CrossEntropyLoss详解](https://blog.csdn.net/Lucinda6/article/details/116162198)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ad41f6",
   "metadata": {},
   "source": [
    "---\n",
    "## 使用高级的API进行训练\n",
    "主要是参数定义的简化，Seq+init模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ee7f32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: train-loss = 115.44628\n",
      "epoch 2: train-loss = 19.59120\n",
      "epoch 3: train-loss = 12.28244\n",
      "epoch 4: train-loss = 8.63245\n",
      "epoch 5: train-loss = 6.89842\n",
      "epoch 6: train-loss = 5.89781\n",
      "epoch 7: train-loss = 4.87764\n",
      "epoch 8: train-loss = 4.25964\n",
      "epoch 9: train-loss = 3.84621\n",
      "epoch 10: train-loss = 3.39110\n"
     ]
    }
   ],
   "source": [
    "# Sequtial + init\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(), # 类似于reshape\n",
    "    nn.Linear(input_size, hidden_size), # input_layer\n",
    "    nn.ReLU(), # 激活函数\n",
    "    nn.Linear(hidden_size, hidden_size) # output_layer\n",
    ")\n",
    "def init_params(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, 0.1)\n",
    "model.apply(init_params)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.03)\n",
    "train(train_iter, 10, model, cross_entropy, optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b8567877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: train-loss = 3.10593\n",
      "epoch 2: train-loss = 2.85907\n",
      "epoch 3: train-loss = 2.58393\n",
      "epoch 4: train-loss = 2.41372\n",
      "epoch 5: train-loss = 2.23298\n",
      "epoch 6: train-loss = 2.11691\n",
      "epoch 7: train-loss = 1.93269\n",
      "epoch 8: train-loss = 1.87558\n",
      "epoch 9: train-loss = 1.73258\n",
      "epoch 10: train-loss = 1.64393\n",
      "Training set: acc=50396, total=60000, accuracy=83.993%\n",
      "Testing set: acc=8133, total=10000, accuracy=81.330%\n"
     ]
    }
   ],
   "source": [
    "train(train_iter, 10, model, cross_entropy, optim)\n",
    "acc, tt = evaluate(train_iter, model)\n",
    "print(f\"Training set: acc={acc}, total={tt}, accuracy={(acc/tt)*100:.3f}%\")\n",
    "acc, tt = evaluate(test_iter, model)\n",
    "print(f\"Testing set: acc={acc}, total={tt}, accuracy={(acc/tt)*100:.3f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
