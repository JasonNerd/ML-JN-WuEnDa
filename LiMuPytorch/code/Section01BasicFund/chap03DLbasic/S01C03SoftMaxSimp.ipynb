{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e5750fb",
   "metadata": {},
   "source": [
    "# softmax回归的简洁实现\n",
    "使用pytorch中的高级API\n",
    "线性回归变得更加容易。 同样，通过深度学习框架的高级API也能更方便地实现softmax回归模型。\n",
    "1. 初始化batch_size=256, 读取train & test数据集\n",
    "2. softmax从输入到输出是一个全连接层, 在本例中输入和输出的维数是784x10，因此们需要在Sequential中添加一个带有10个输出的全连接层，我们仍然以均值0和标准差0.01随机初始化权重。另一方面，原始输入是28x28的灰度值矩阵，因此需要展平为向量，即flatten层\n",
    "3. **交叉熵损失**。预测值在对应分类的负对数值，预测值通过特征的线性组合外套一个softmax操作得到，即$\\hat y_j = \\frac{\\exp(o_j)}{\\sum_k \\exp(o_k)}$。这里其实有个scaling的问题, 假设$o_k$中的一些数值非常大，$\\exp(o_k)$也会很大, 可能上溢, 出现inf,nan等情况.一种思路是$$\\begin{split}\\begin{aligned}\n",
    "\\hat y_j & =  \\frac{\\exp(o_j - \\max(o_k))\\exp(\\max(o_k))}{\\sum_k \\exp(o_k - \\max(o_k))\\exp(\\max(o_k))} \\\\\n",
    "& = \\frac{\\exp(o_j - \\max(o_k))}{\\sum_k \\exp(o_k - \\max(o_k))}.\n",
    "\\end{aligned}\\end{split}$$另一方面损失函数(注意取其相反数)为$$\\begin{split}\\begin{aligned}\n",
    "\\log{(\\hat y_j)} & = \\log\\left( \\frac{\\exp(o_j - \\max(o_k))}{\\sum_k \\exp(o_k - \\max(o_k))}\\right) \\\\\n",
    "& = \\log{(\\exp(o_j - \\max(o_k)))}-\\log{\\left( \\sum_k \\exp(o_k - \\max(o_k)) \\right)} \\\\\n",
    "& = o_j - \\max(o_k) -\\log{\\left( \\sum_k \\exp(o_k - \\max(o_k)) \\right)}.\n",
    "\\end{aligned}\\end{split}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862be383",
   "metadata": {},
   "source": [
    "---\n",
    "4. 优化函数选择SGD.实际上交叉熵损失也是一行代码\n",
    "5. 进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a363d29c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 1, 28, 28])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import FashionMNIST\n",
    "# 1. load data \n",
    "batch_size = 256\n",
    "trans = transforms.ToTensor()\n",
    "train_minist = FashionMNIST(root=\"../../data\", train=True, transform=trans)\n",
    "test_minist = FashionMNIST(root=\"../../data\", train=False, transform=trans)\n",
    "train_iter = DataLoader(train_minist, batch_size, shuffle=True, num_workers=6)\n",
    "test_iter = DataLoader(test_minist, batch_size, shuffle=False, num_workers=6)\n",
    "next(iter(train_iter))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6aa84571",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'softmax' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [4], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m sam_tot \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m test_iter:\n\u001b[1;32m---> 30\u001b[0m     acc_cnt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m cmpY(softmax(X), y)\n\u001b[0;32m     31\u001b[0m     sam_tot \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(y)\n\u001b[0;32m     32\u001b[0m acc_cnt, sam_tot, acc_cnt\u001b[38;5;241m/\u001b[39msam_tot\n",
      "\u001b[1;31mNameError\u001b[0m: name 'softmax' is not defined"
     ]
    }
   ],
   "source": [
    "# 2. 定义模型\n",
    "from torch import nn\n",
    "num_inputs = 28*28\n",
    "num_outputs = 10\n",
    "net = nn.Sequential(nn.Flatten(), nn.Linear(num_inputs, num_outputs))\n",
    "# 3. 参数初始化 -- 这里将采用常用的初始化方法 apply\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.03)\n",
    "net.apply(init_weights)\n",
    "# 3. 损失函数\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "# 4. 优化函数\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=0.03)\n",
    "# 5. 进行训练\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in train_iter:\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y)\n",
    "        trainer.zero_grad()  # 注意将原先的计算图清空\n",
    "        l.mean().backward()  # 反向传播\n",
    "        trainer.step()  # 更新梯度\n",
    "    \n",
    "# 6. 计算精度\n",
    "def cmpY(y_hat, y):\n",
    "    return (y == y_hat.argmax(dim=1)).sum()\n",
    "acc_cnt = 0\n",
    "sam_tot = 0\n",
    "for X, y in test_iter:\n",
    "    acc_cnt += cmpY(softmax(X), y)\n",
    "    sam_tot += len(y)\n",
    "acc_cnt, sam_tot, acc_cnt/sam_tot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e55c33",
   "metadata": {},
   "source": [
    "1. [nn.init 中实现的初始化函数 uniform, normal, const](https://cloud.tencent.com/developer/article/1627511)\n",
    "<br>\n",
    "2. [每天学点pytorch--torch.nn.Module的apply()方法](https://blog.csdn.net/qiumokucao/article/details/121356553)apply(fn)的官网介绍，该方法会将fn递归的应用于模块的每一个子模块（.children()的结果）及其自身。典型的用法是，对一个model的参数进行初始化\n",
    "<br>\n",
    "3. [nn.CrossEntropyLoss]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0214d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
