{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "595fffbc",
   "metadata": {},
   "source": [
    "假设有4个特征，1000条数据，自己把完整过程跑一遍\n",
    "1. y = $w^T$x+b, $w=[w_1, w_2, w_3, w_4]$, $b$, 加入正态分布随机噪声，生成1000条数据\n",
    "2. 编写小批量抽样迭代器\n",
    "3. 编写假设函数，也即线性回归函数的原型\n",
    "4. 编写代价函数，平方损失函数\n",
    "5. 编写优化函数，梯度下降算法\n",
    "6. 初始化batch_size, epochs, lr, 随机初始化w, b, 进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4d09fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 4]),\n",
       " torch.Size([1000, 1]),\n",
       " tensor([ 1.0819,  1.2127,  0.5733, -0.1907]),\n",
       " tensor([4.8329]),\n",
       " tensor(4.8405))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "# 1. 生成1000条3维的模拟数据\n",
    "w_true = torch.tensor([1.2, 0.9, 2, 2.6])\n",
    "b_true = 1.8\n",
    "\n",
    "def linearDataGenerator(w, b, n):\n",
    "    X = torch.normal(0, 1, (n, len(w)))\n",
    "    y = torch.matmul(X, w) + b\n",
    "    y += torch.normal(0, 0.01, y.shape)\n",
    "    return X, y.reshape(-1, 1)\n",
    "\n",
    "n = 1000\n",
    "X, y = linearDataGenerator(w_true, b_true, n)\n",
    "X.shape, y.shape, X[0, :], y[0, :], torch.sum(X[0, :]*w_true)+1.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d8edfa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.4365,  2.0285,  0.0843, -1.9745],\n",
       "         [ 0.7600,  0.6829,  0.2133, -0.5305],\n",
       "         [-0.9729,  1.4921,  1.2595,  2.0410],\n",
       "         [ 2.2248, -0.5150, -2.0671,  0.1570],\n",
       "         [-0.7623, -1.3450, -0.1716, -1.0110]]),\n",
       " tensor([[-1.8674],\n",
       "         [ 2.3755],\n",
       "         [ 9.8184],\n",
       "         [ 0.2783],\n",
       "         [-3.2847]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. 接下来需要一个迭代器，它接受X, y返回一个batch包含训练样本和标签\n",
    "def batch_iter(X, y, batch_size):\n",
    "    n = X.shape[0]\n",
    "    indices = list(range(n))\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, n, batch_size):\n",
    "        bie = torch.tensor(indices[i: min(i+batch_size, n)])\n",
    "        yield X[bie, :], y[bie, :]\n",
    "\n",
    "# 测试一下迭代器\n",
    "g = batch_iter(X, y, 5)\n",
    "next(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5d7cdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 接下来三大函数\n",
    "def linreg(X, w, b):\n",
    "    return torch.matmul(X, w)+b\n",
    "def lrcost(y_hat, y):\n",
    "    return (1/2)*(y-y_hat)**2\n",
    "def sgd(params, lr, batch_size):\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr*param.grad/batch_size\n",
    "            param.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efcc62d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 1, train loss is 0.056273\n",
      "In epoch 2, train loss is 0.000312\n",
      "In epoch 3, train loss is 0.000050\n",
      "In epoch 4, train loss is 0.000049\n",
      "In epoch 5, train loss is 0.000049\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[1.1998],\n",
       "         [0.9001],\n",
       "         [1.9997],\n",
       "         [2.6002]], requires_grad=True),\n",
       " tensor([1.7999], requires_grad=True))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 然后初始化参数开始训练\n",
    "d = 4\n",
    "n = 1000\n",
    "w = torch.normal(0, 1, (4, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "epochs = 5\n",
    "lr = 0.03\n",
    "batch_size = 12\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batchX, batchy in batch_iter(X, y, batch_size):\n",
    "        bl = lrcost(batchy, linreg(batchX, w, b))\n",
    "        bl.sum().backward()\n",
    "        sgd([w, b], lr, batch_size)\n",
    "    with torch.no_grad():\n",
    "        tl = lrcost(y, linreg(X, w, b))\n",
    "        print(f\"In epoch {epoch+1}, train loss is {float(tl.mean()):.6f}\")\n",
    "w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5eaeff",
   "metadata": {},
   "source": [
    "## 使用高级的API进行训练\n",
    "也即有些事情可以调用库函数，这样使得工作更加简洁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6451611f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-1.6472,  0.1517,  0.1393,  0.6966],\n",
       "         [ 1.1862,  0.7840, -0.2184,  1.4221],\n",
       "         [ 0.8217,  1.9041, -2.1371,  1.8043],\n",
       "         [-0.2713,  0.0870,  0.9319,  1.1358],\n",
       "         [-0.2665,  0.9629,  0.9499, -0.6686]]),\n",
       " tensor([[2.0581],\n",
       "         [7.1650],\n",
       "         [4.9126],\n",
       "         [6.3774],\n",
       "         [2.5151]])]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第一步生成数据还得靠自己，假设得到了用于训练的X[n, d]和y[n, 1]\n",
    "# 第二步构造迭代器就有api了\n",
    "from torch.utils import data\n",
    "\n",
    "def getBatch(X, y, batch_size, is_train):\n",
    "    # TensorDataset相当于打包, 传入值均为tensor, 第一维度必须相等\n",
    "    dataset = data.TensorDataset(X, y)\n",
    "    # DataLoader载入数据，完成迭代器的构造, is_train=True表示训练时是需要打乱的\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "batch_iter = iter(getBatch(X, y, 5, True))\n",
    "next(batch_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6316b3e0",
   "metadata": {},
   "source": [
    "**使用标准深度学习模型**\n",
    "对于标准深度学习模型，我们可以使用框架的预定义好的层。这使我们只需关注使用哪些层来构造模型，而不必关注层的实现细节。 我们首先定义一个模型变量net，它是一个Sequential类的实例。 Sequential类将多个层串联在一起。 当给定输入数据时，Sequential实例将数据传入到第一层， 然后将第一层的输出作为第二层的输入，以此类推。\n",
    "线性回归模型可以抽象为单层网络架构， 这一单层被称为$全连接层（fully-connected-layer）$， 因为它的每一个输入都通过矩阵-向量乘法得到它的每个输出。\n",
    "\n",
    "**在PyTorch中，全连接层在Linear类中定义**。 值得注意的是，我们将两个参数传递到nn.Linear中。 第一个指定输入特征形状，第二个指定输出特征形状，输出特征形状为单个标量，因此为1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bbbecbd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Sequential(\n",
       "   (0): Linear(in_features=4, out_features=1, bias=True)\n",
       " ),\n",
       " torch.nn.modules.container.Sequential)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "d = 4 # 输入特征数\n",
    "o = 1 # 输出值维度，由于是连续值标量，故为1\n",
    "net = nn.Sequential(nn.Linear(d, o))\n",
    "net, type(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aadde22",
   "metadata": {},
   "source": [
    "正如我们在构造nn.Linear时指定输入和输出尺寸一样， 现在我们能直接访问参数以设定它们的初始值。 我们通过net[0]选择网络中的第一个图层， 然后使用weight.data和bias.data方法访问参数。 我们还可以使用替换方法normal_和fill_来重写参数值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4c3a020a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0014, -0.0052,  0.0045, -0.0215]]),\n",
       " tensor([0.]),\n",
       " torch.Size([1000, 4]),\n",
       " torch.Size([1000, 1]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data.normal_(0, 0.01)\n",
    "net[0].bias.data.fill_(0)\n",
    "net[0].weight.data, net[0].bias.data, X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b8c112a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: 0.000105\n",
      "epoch 2: 0.000099\n",
      "epoch 3: 0.000099\n"
     ]
    }
   ],
   "source": [
    "# 以上实际是假设函数的构造（神经网络的搭建）过程\n",
    "loss = nn.MSELoss() # 损失函数\n",
    "optim = torch.optim.SGD(params=net.parameters(), lr=0.03) # 优化函数\n",
    "batch_iter = getBatch(X, y, 5, True)\n",
    "# 以下开始训练\n",
    "epoch_num = 3\n",
    "for epoch in range(epoch_num):\n",
    "    for dX, dy in batch_iter:\n",
    "        l = loss(net(dX), dy)\n",
    "        optim.zero_grad()\n",
    "        l.backward()\n",
    "        optim.step()\n",
    "    print(f\"epoch {epoch+1}: {float(loss(net(X), y)):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fd2967b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.1996, 0.9010, 1.9988, 2.6000]]), tensor([1.8000]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data, net[0].bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e17b3f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on SGD in module torch.optim.sgd object:\n",
      "\n",
      "class SGD(torch.optim.optimizer.Optimizer)\n",
      " |  SGD(params, lr=<required parameter>, momentum=0, dampening=0, weight_decay=0, nesterov=False, *, maximize=False, foreach: Union[bool, NoneType] = None)\n",
      " |  \n",
      " |  Implements stochastic gradient descent (optionally with momentum).\n",
      " |  \n",
      " |  .. math::\n",
      " |     \\begin{aligned}\n",
      " |          &\\rule{110mm}{0.4pt}                                                                 \\\\\n",
      " |          &\\textbf{input}      : \\gamma \\text{ (lr)}, \\: \\theta_0 \\text{ (params)}, \\: f(\\theta)\n",
      " |              \\text{ (objective)}, \\: \\lambda \\text{ (weight decay)},                          \\\\\n",
      " |          &\\hspace{13mm} \\:\\mu \\text{ (momentum)}, \\:\\tau \\text{ (dampening)},\n",
      " |          \\:\\textit{ nesterov,}\\:\\textit{ maximize}                                     \\\\[-1.ex]\n",
      " |          &\\rule{110mm}{0.4pt}                                                                 \\\\\n",
      " |          &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n",
      " |          &\\hspace{5mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})           \\\\\n",
      " |          &\\hspace{5mm}\\textbf{if} \\: \\lambda \\neq 0                                           \\\\\n",
      " |          &\\hspace{10mm} g_t \\leftarrow g_t + \\lambda  \\theta_{t-1}                            \\\\\n",
      " |          &\\hspace{5mm}\\textbf{if} \\: \\mu \\neq 0                                               \\\\\n",
      " |          &\\hspace{10mm}\\textbf{if} \\: t > 1                                                   \\\\\n",
      " |          &\\hspace{15mm} \\textbf{b}_t \\leftarrow \\mu \\textbf{b}_{t-1} + (1-\\tau) g_t           \\\\\n",
      " |          &\\hspace{10mm}\\textbf{else}                                                          \\\\\n",
      " |          &\\hspace{15mm} \\textbf{b}_t \\leftarrow g_t                                           \\\\\n",
      " |          &\\hspace{10mm}\\textbf{if} \\: \\textit{nesterov}                                       \\\\\n",
      " |          &\\hspace{15mm} g_t \\leftarrow g_{t} + \\mu \\textbf{b}_t                             \\\\\n",
      " |          &\\hspace{10mm}\\textbf{else}                                                   \\\\[-1.ex]\n",
      " |          &\\hspace{15mm} g_t  \\leftarrow  \\textbf{b}_t                                         \\\\\n",
      " |          &\\hspace{5mm}\\textbf{if} \\: \\textit{maximize}                                          \\\\\n",
      " |          &\\hspace{10mm}\\theta_t \\leftarrow \\theta_{t-1} + \\gamma g_t                   \\\\[-1.ex]\n",
      " |          &\\hspace{5mm}\\textbf{else}                                                    \\\\[-1.ex]\n",
      " |          &\\hspace{10mm}\\theta_t \\leftarrow \\theta_{t-1} - \\gamma g_t                   \\\\[-1.ex]\n",
      " |          &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n",
      " |          &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n",
      " |          &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n",
      " |     \\end{aligned}\n",
      " |  \n",
      " |  Nesterov momentum is based on the formula from\n",
      " |  `On the importance of initialization and momentum in deep learning`__.\n",
      " |  \n",
      " |  Args:\n",
      " |      params (iterable): iterable of parameters to optimize or dicts defining\n",
      " |          parameter groups\n",
      " |      lr (float): learning rate\n",
      " |      momentum (float, optional): momentum factor (default: 0)\n",
      " |      weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
      " |      dampening (float, optional): dampening for momentum (default: 0)\n",
      " |      nesterov (bool, optional): enables Nesterov momentum (default: False)\n",
      " |      maximize (bool, optional): maximize the params based on the objective, instead of\n",
      " |          minimizing (default: False)\n",
      " |      foreach (bool, optional): whether foreach implementation of optimizer\n",
      " |          is used (default: None)\n",
      " |  \n",
      " |  Example:\n",
      " |      >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
      " |      >>> optimizer.zero_grad()\n",
      " |      >>> loss_fn(model(input), target).backward()\n",
      " |      >>> optimizer.step()\n",
      " |  \n",
      " |  __ http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf\n",
      " |  \n",
      " |  .. note::\n",
      " |      The implementation of SGD with Momentum/Nesterov subtly differs from\n",
      " |      Sutskever et. al. and implementations in some other frameworks.\n",
      " |  \n",
      " |      Considering the specific case of Momentum, the update can be written as\n",
      " |  \n",
      " |      .. math::\n",
      " |          \\begin{aligned}\n",
      " |              v_{t+1} & = \\mu * v_{t} + g_{t+1}, \\\\\n",
      " |              p_{t+1} & = p_{t} - \\text{lr} * v_{t+1},\n",
      " |          \\end{aligned}\n",
      " |  \n",
      " |      where :math:`p`, :math:`g`, :math:`v` and :math:`\\mu` denote the\n",
      " |      parameters, gradient, velocity, and momentum respectively.\n",
      " |  \n",
      " |      This is in contrast to Sutskever et. al. and\n",
      " |      other frameworks which employ an update of the form\n",
      " |  \n",
      " |      .. math::\n",
      " |          \\begin{aligned}\n",
      " |              v_{t+1} & = \\mu * v_{t} + \\text{lr} * g_{t+1}, \\\\\n",
      " |              p_{t+1} & = p_{t} - v_{t+1}.\n",
      " |          \\end{aligned}\n",
      " |  \n",
      " |      The Nesterov version is analogously modified.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      SGD\n",
      " |      torch.optim.optimizer.Optimizer\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, params, lr=<required parameter>, momentum=0, dampening=0, weight_decay=0, nesterov=False, *, maximize=False, foreach: Union[bool, NoneType] = None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  step(self, closure=None)\n",
      " |      Performs a single optimization step.\n",
      " |      \n",
      " |      Args:\n",
      " |          closure (callable, optional): A closure that reevaluates the model\n",
      " |              and returns the loss.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.optim.optimizer.Optimizer:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  add_param_group(self, param_group)\n",
      " |      Add a param group to the :class:`Optimizer` s `param_groups`.\n",
      " |      \n",
      " |      This can be useful when fine tuning a pre-trained network as frozen layers can be made\n",
      " |      trainable and added to the :class:`Optimizer` as training progresses.\n",
      " |      \n",
      " |      Args:\n",
      " |          param_group (dict): Specifies what Tensors should be optimized along with group\n",
      " |              specific optimization options.\n",
      " |  \n",
      " |  load_state_dict(self, state_dict)\n",
      " |      Loads the optimizer state.\n",
      " |      \n",
      " |      Args:\n",
      " |          state_dict (dict): optimizer state. Should be an object returned\n",
      " |              from a call to :meth:`state_dict`.\n",
      " |  \n",
      " |  state_dict(self)\n",
      " |      Returns the state of the optimizer as a :class:`dict`.\n",
      " |      \n",
      " |      It contains two entries:\n",
      " |      \n",
      " |      * state - a dict holding current optimization state. Its content\n",
      " |          differs between optimizer classes.\n",
      " |      * param_groups - a list containing all parameter groups where each\n",
      " |          parameter group is a dict\n",
      " |  \n",
      " |  zero_grad(self, set_to_none: bool = False)\n",
      " |      Sets the gradients of all optimized :class:`torch.Tensor` s to zero.\n",
      " |      \n",
      " |      Args:\n",
      " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      " |              This will in general have lower memory footprint, and can modestly improve performance.\n",
      " |              However, it changes certain behaviors. For example:\n",
      " |              1. When the user tries to access a gradient and perform manual ops on it,\n",
      " |              a None attribute or a Tensor full of 0s will behave differently.\n",
      " |              2. If the user requests ``zero_grad(set_to_none=True)`` followed by a backward pass, ``.grad``\\ s\n",
      " |              are guaranteed to be None for params that did not receive a gradient.\n",
      " |              3. ``torch.optim`` optimizers have a different behavior if the gradient is 0 or None\n",
      " |              (in one case it does the step with a gradient of 0 and in the other it skips\n",
      " |              the step altogether).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.optim.optimizer.Optimizer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b266c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
