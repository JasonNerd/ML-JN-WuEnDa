{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1e38a32",
   "metadata": {},
   "source": [
    "# 数据的存储与操作\n",
    "为了完成对数据的操作，必须要有一种合适的形式将数据存储在计算机中，通常形式为n维的数组，在python语言中，在机器学习领域，他的名字叫做**张量(tensor)**。torch.tensor具备许多重要的的功能，例如**自动微分(auto-grad)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80802a58",
   "metadata": {},
   "source": [
    "## 1. 张量(tensor)的性质与构造\n",
    "张量表示由一个数值组成的数组，这个数组可能有多个维度。 具有一个轴的张量对应数学上的**向量（vector）**； 具有两个轴的张量对应数学上的**矩阵（matrix）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba6621a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(12)  # arange()\n",
    "x # 是一个一维的tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d937e736",
   "metadata": {},
   "source": [
    "可以通过$shape$**属性**访问tensor实例的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45d335ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd3d915",
   "metadata": {},
   "source": [
    "注意x是一个行向量，若无特别指定，x将放在内存中，并且使用CPU进行运算操作\n",
    "如果只想获取tensor的元素个数，可以使用$tensor.numel()$**函数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daf59b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1023af54",
   "metadata": {},
   "source": [
    "$tensor.reshape(a, b)$, 将tensor的形状改为axb的形状，其中行或列可以缺省，用-1代替，另外，reshape函数的参数也可以是元组，例如reshape((a, b, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72c71499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X34 = x.reshape(3, -1)\n",
    "X34"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec42536",
   "metadata": {},
   "source": [
    "有时我们需要得到**特定形状的元素全0或全1的张量**，或者**特定形状的随机初始化的张量**，使用函数zeros(a, b)或者ones(a, b)或者randn(a, b)(注：随机采样自标准正态分布，范围在0-1之间，可依据实际调整数据范围)即可，同样这里也可以是元组。另外，我们也可以使用python的(嵌套)列表(list)来为tensor赋初值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daf74221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.]]]),\n",
       " tensor([[[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]]]),\n",
       " tensor([[[-1.4777, -0.8771,  0.8572],\n",
       "          [-0.7417,  1.2801, -0.3386],\n",
       "          [-2.1799,  0.5772,  0.0328]],\n",
       " \n",
       "         [[ 0.7768,  1.2282,  0.7051],\n",
       "          [-0.6947,  2.6272,  0.6684],\n",
       "          [ 0.5836, -0.0804,  0.7068]]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X0 = torch.ones((2, 3, 3))\n",
    "X1 = torch.zeros((2, 3, 3))\n",
    "Xr = torch.randn((2, 3, 3))\n",
    "X0, X1, Xr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca2db8a",
   "metadata": {},
   "source": [
    "## 2. 张量的四则运算\n",
    "这里主要是相同形状的tensor按元素对应的进行标量运算，例如$+, -, *, /, **, \\% $，以及torch.exp(), torch.log()等，向量点积与矩阵运算等线性代数操作放在后面讲."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82e14c79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3., 4., 6., 5.]),\n",
       " tensor([-1.,  2.,  2., -1.]),\n",
       " tensor([2., 3., 8., 6.]),\n",
       " tensor([0.5000, 3.0000, 2.0000, 0.6667]),\n",
       " tensor([ 1.,  3., 16.,  8.]),\n",
       " tensor([1., 0., 0., 2.]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 3.0, 4, 2])\n",
    "y = torch.tensor([2, 1, 2, 3])\n",
    "x+y, x-y, x*y, x/y, x**y, x%y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfd09e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 2.7183, 20.0855, 54.5981,  7.3891]),\n",
       " tensor([0.0000, 1.0986, 1.3863, 0.6931]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(x), torch.log(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47586ab2",
   "metadata": {},
   "source": [
    "此外还可以多个张量连结$concatenate$在一起，使用$cat$函数，对于矩阵，我们可以沿行（轴-0，形状的第一个元素） 和按列（轴-1，形状的第二个元素）连结。以及$sum$对张量中的元素求和（得到单元tensor）。(注意这个dim是有取值范围的，例如假设x,y都是行向量，那么他们只能在行方向上拓展，也即拓展后还是一个行向量。)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5301b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 2],\n",
       "         [3, 4, 5],\n",
       "         [2, 3, 4],\n",
       "         [3, 2, 1]]),\n",
       " tensor([[0, 1, 2, 2, 3, 4],\n",
       "         [3, 4, 5, 3, 2, 1]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(6).reshape((2, 3))\n",
    "Y = torch.tensor([2, 3, 4, 3, 2, 1]).reshape((2, 3))\n",
    "torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e75cd299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccc76fd",
   "metadata": {},
   "source": [
    "## 3. 广播\n",
    "在某些情况下，即使形状不同，我们仍然可以通过调用**广播机制(broadcasting mechanism)**来执行按元素操作.在大多数情况下，我们将沿着数组中长度为1的轴进行广播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f0b27d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2, 5, 7]]),\n",
       " tensor([[3],\n",
       "         [4]]),\n",
       " tensor([[ 5,  8, 10],\n",
       "         [ 6,  9, 11]]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([2, 5, 7]).reshape((1, 3))\n",
    "Y = torch.tensor([3, 4]).reshape((2, 1))\n",
    "X, Y, X+Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7f634c",
   "metadata": {},
   "source": [
    "## 4. 索引和切片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a6970b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 2],\n",
       "         [3, 4, 5]]),\n",
       " tensor([3, 4, 5]),\n",
       " tensor([2, 5]),\n",
       " tensor([[0, 1],\n",
       "         [3, 4]]),\n",
       " tensor(4))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(6).reshape((2,3))\n",
    "# 取1行，例如第二行\n",
    "X_row2 = X[1, :]\n",
    "# 取一列，例如第三列\n",
    "X_col3 = X[:, 2]\n",
    "# 取一个元素，例如第2行第2列\n",
    "x_22 = X[1, 1]\n",
    "# 取一块，例如，第1，2行和第1，2列\n",
    "X_22 = X[0:2, 0:2]\n",
    "X, X_row2, X_col3, X_22,x_22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a005c76",
   "metadata": {},
   "source": [
    "## 5. 内存的消耗\n",
    "1. Y = X + Y, 将加法结果重新赋值给Y后，Y的地址将改变\n",
    "2. Y[:] = X + Y\n",
    "3. Y += X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "59cae66f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用id表征数据地址\n",
    "X = torch.tensor([1, 3, 5, 7, 9, 6]).reshape((2,3))\n",
    "Y = torch.tensor([2, 4, 4, 6, 3, 8]).reshape((2,3))\n",
    "before = id(Y)\n",
    "Y = X + Y # t <- X + Y, Y <- t, so we allocate new memory in this statement\n",
    "before == id(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7933ceec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(Z)= 2117180683184\n",
      "id(Z)= 2117180683184\n"
     ]
    }
   ],
   "source": [
    "# 对于第二个操作，地址会改变吗？\n",
    "Z = torch.zeros_like(Y)\n",
    "print(\"id(Z)=\", id(Z))\n",
    "Z[:] = X + Y\n",
    "print(\"id(Z)=\", id(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "017664e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2117181327536\n",
      "2117181327536\n"
     ]
    }
   ],
   "source": [
    "# 对于第三个操作，地址会改变吗？\n",
    "print(id(X))\n",
    "X += Y\n",
    "print(id(X))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
