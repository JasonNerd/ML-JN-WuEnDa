{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50e194eb",
   "metadata": {},
   "source": [
    "# 从0开始的RNN-文本预处理\n",
    "例如，一篇文章可以被简单地看作是一串单词序列，甚至是一串字符序列。 本节中，我们将解析文本的常见预处理步骤。 这些步骤通常包括：\n",
    "1. 将文本作为字符串加载到内存中。\n",
    "2. 将字符串拆分为词元（如单词和字符）。\n",
    "3. 建立一个词表，将拆分的词元映射到数字索引。\n",
    "4. 将文本转换为数字索引序列，方便模型操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ecb3d1",
   "metadata": {},
   "source": [
    "## 读取文本行\n",
    "url: https://www.gutenberg.org/ebooks/35  \n",
    "离线文件相对地址: ../data/TimeMachine.txt\n",
    "涉及知识点-python正则匹配  \n",
    "re.sub(pattern, rep, inp) 是一种带正则的字符串替换函数，功能是将输入字符串inp中所有与pattern模式匹配的子串替换为rep，例如`re.sub(\"\\d+\", \"6\", inp)`就是把字符串中所有的数字都换为6.  \n",
    "[^a-z]可以匹配任何不在“a”到“z”范围内的任意字符。\n",
    "参考链接:  \n",
    "[python re模块(正则表达式) sub()函数详解](https://blog.csdn.net/qq_43088815/article/details/90214217)  \n",
    "[正则表达式符号大全](https://blog.csdn.net/wujunlei1595848/article/details/81316800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2b95bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3557,\n",
       " 'the project gutenberg ebook of the time machine by h g wells',\n",
       " '',\n",
       " 'this ebook is for the use of anyone anywhere in the united states and')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取文件, 转为一个字符串行列表, 去掉除字母空格以外的字符并统一转为小写\n",
    "import re\n",
    "def read_text(url=None):\n",
    "    if not url:\n",
    "        url = \"../data/TheTimeMachine.txt\"\n",
    "    with open(url, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    return [re.sub(r\"[^a-zA-Z]+\", \" \", line).strip().lower() for line in lines]\n",
    "lines = read_text()\n",
    "len(lines), lines[0], lines[1], lines[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a375588",
   "metadata": {},
   "source": [
    "## 词元(token)列表\n",
    "以上，将文本的每一行作为一个字符串读入了内存，并将所有非字母字符替换为空格，最后去掉首尾多余的空格，并将所有字母转为小写。以下是把列表中的每一个元素--一行（看做一个字符串）转为一个**词元(token)列表**，它可以是一个单词或者字符，默认是单词。下面的tokenize函数将文本行列表（lines）作为输入， 列表中的每个元素是一个文本序列（如一条文本行）。 每个文本序列又被拆分成一个词元列表，词元（token）是文本的基本单位。 最后，返回一个由词元列表组成的列表，其中的每个词元都是一个字符串（string）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bbe9b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(lines, token=\"word\"):\n",
    "    if token == \"word\":\n",
    "        return [line.split() for line in lines]\n",
    "    if token == \"char\":\n",
    "        return [list(line) for line in lines]\n",
    "tokens = tokenize(lines)\n",
    "print(tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944dbdf0",
   "metadata": {},
   "source": [
    "## 词表建立\n",
    "词元的类型是字符串，而模型需要的输入是数字，因此这种类型不方便模型使用。 现在，让我们构建一个字典，通常也叫做词表（vocabulary）， 用来将字符串类型的词元映射到从0开始的数字索引中。 我们先将训练集中的所有文档合并在一起，对它们的唯一词元进行统计， 得到的统计结果称之为语料（corpus）。 然后根据每个唯一词元的出现频率，为其分配一个数字索引。 很少出现的词元通常被移除，这可以降低复杂性。 另外，语料库中不存在或已删除的任何词元都将映射到一个特定的未知词元“< unk >”。 我们可以选择增加一个列表，用于保存那些被保留的词元， 例如：填充词元（“< pad >”）； 序列开始词元（“< bos >”）； 序列结束词元（“< eos >”）    \n",
    "\n",
    "[Python collections.Counter()用法](https://blog.csdn.net/qwe1257/article/details/83272340)  \n",
    "collections在python官方文档中的解释是High-performance container datatypes，直接的中文翻译解释高性能容量数据类型。其中Counter中文意思是计数器，也就是我们常用于统计的一种数据类型，在使用Counter之后可以让我们的代码更加简单易读。  \n",
    "\n",
    "[Python @property装饰器详解](http://c.biancheng.net/view/4561.html)  \n",
    "既要保护类的封装特性，又要让开发者可以使用“对象.属性”的方式操作操作类属性，Python提供了 @property 装饰器。通过 @property 装饰器，可以直接通过方法名来得到属性值（方法返回值），不需要在方法名后添加一对“（）”小括号。 \n",
    "\n",
    "[Python 字典(Dictionary) get()方法](https://www.runoob.com/python/att-dictionary-get.html)  \n",
    "Python 字典(Dictionary) get() 函数返回指定键的值。\n",
    "dict.get(key[, value])   \n",
    "key -- 字典中要查找的键。  \n",
    "value -- 可选，如果指定键的值不存在时，返回该默认值。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ddc6b1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "class Vocab:\n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "        # 将tokens展平为一个单词列表，对其中的单词进行计数并按词频倒序排序\n",
    "        counter = count_corpus(tokens)\n",
    "        self._token_freqs = sorted(counter.items(), \n",
    "                                   key=lambda x: x[1], reverse=True)\n",
    "        # id->token, token->id\n",
    "        # 先处理保留词元，例如未定义词语\n",
    "        self.id2token = ['<unk>']+reserved_tokens\n",
    "        self.token2id = {token: idx for idx, token in enumerate(self.id2token)}\n",
    "        # 接下来对_token_freqs进行处理\n",
    "        for token, freq in self._token_freqs:\n",
    "            if freq < min_freq:\n",
    "                break\n",
    "            self.id2token.append(token)\n",
    "            self.token2id[token]=len(self.id2token)-1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.id2token)\n",
    "    # 通过token获取id, 注意这是一个递归操作, 也即是可以传入列表的\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            # dict().get(key, default_value)\n",
    "            return self.token2id.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.id2token[indices]\n",
    "        return [self.to_tokens[index] for index in indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):  # 未知词元的索引为0\n",
    "        return 0\n",
    "    # @property使得外部类可以通过函数名访问私有属性\n",
    "    @property\n",
    "    def token_freqs(self):\n",
    "        return self._token_freqs    \n",
    "\n",
    "def count_corpus(tokens):\n",
    "    \"\"\"词频计数\"\"\"\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c223f32",
   "metadata": {},
   "source": [
    "dict.items()\n",
    "https://www.runoob.com/python/att-dictionary-items.html  \n",
    "以列表返回可遍历的(键, 值) 元组数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "631ad169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<unk>', 0),\n",
       " ('the', 1),\n",
       " ('and', 2),\n",
       " ('of', 3),\n",
       " ('i', 4),\n",
       " ('a', 5),\n",
       " ('to', 6),\n",
       " ('in', 7),\n",
       " ('was', 8),\n",
       " ('that', 9)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = Vocab(tokens)\n",
    "list(vocab.token2id.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61eedf64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 2477),\n",
       " ('and', 1312),\n",
       " ('of', 1286),\n",
       " ('i', 1268),\n",
       " ('a', 877),\n",
       " ('to', 766),\n",
       " ('in', 606),\n",
       " ('was', 554),\n",
       " ('that', 458),\n",
       " ('it', 452)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab.token_freqs)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a984dfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
      "[1, 53, 44, 314, 3, 1, 19, 46, 33, 1163, 1164, 360]\n",
      "[]\n",
      "[]\n",
      "['this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'united', 'states', 'and']\n",
      "[21, 314, 29, 17, 1, 220, 3, 558, 1165, 7, 1, 268, 235, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('flower', 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(tokens[i])\n",
    "    print(vocab[tokens[i]])\n",
    "vocab.token_freqs[vocab[\"flower\"]-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d693906e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus_time_machine(max_tokens=-1):\n",
    "    lines = read_text()\n",
    "    tokens = tokenize(lines, token=\"char\")\n",
    "    vocab = Vocab(tokens)\n",
    "    if max_tokens > 0:\n",
    "        corpus = corpus[:max_tokens]\n",
    "    corups = [vocab[token] for line in tokens for token in line]\n",
    "    return corups, vocab\n",
    "corups, vocab = load_corpus_time_machine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fdebce21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(189663, 28)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corups), len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f6702412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', ' ', 'e', 't', 'a', 'i', 'o', 'n', 's', 'r']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.id2token[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f2bff18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 32926), ('e', 19781), ('t', 15155), ('a', 12752), ('i', 11312)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab.token_freqs)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86b3751c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 't', 't', 3)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l0 = corups[0]\n",
    "t0 = vocab.id2token[l0]\n",
    "t0a = vocab.to_tokens(l0)\n",
    "l0a = vocab[t0]\n",
    "l0, t0, t0a, l0a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f4441f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
