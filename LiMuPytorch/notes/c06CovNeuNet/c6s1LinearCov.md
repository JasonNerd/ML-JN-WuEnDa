# 从全连接层到卷积
我们之前讨论的多层感知机十分适合处理表格数据，其中行对应样本，列对应特征。 对于表格数据，我们寻找的模式可能涉及特征之间的交互，但是我们不能预先假设任何与特征交互相关的先验结构。 此时，多层感知机可能是最好的选择，然而对于高维感知数据，这种缺少结构的网络可能会变得不实用。 **(卷积神经网络convolutional neural networks，CNN)** 是机器学习利用自然图像中一些已知结构的创造性方法。

现在，我们将上述想法总结一下，从而帮助我们设计适合于计算机视觉的神经网络架构：
**平移不变性（translation invariance）**：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。
**局部性（locality）**：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。

## 平移不变性
使用X[i, j]和H[i, j]分别表示输入图像和隐藏表示中位置（i, j）处的像素。 为了使每个隐藏神经元都能接收到每个输入像素的信息，我们将参数从权重矩阵（如同我们先前在多层感知机中所做的那样）替换为四阶权重张量W。假设U包含偏置参数，我们可以将全连接层形式化地表示为
$$
\begin{split}\begin{aligned} \left[\mathbf{H}\right]_{i, j} &= [\mathbf{U}]_{i, j} + \sum_k \sum_l[\mathsf{W}]_{i, j, k, l}  [\mathbf{X}]_{k, l}\\ &=  [\mathbf{U}]_{i, j} +
\sum_a \sum_b [\mathsf{V}]_{i, j, a, b}  [\mathbf{X}]_{i+a, j+b}.\end{aligned}\end{split}
$$
其中，从W到V的转换只是形式上的转换，因为在这两个四阶张量的元素之间存在一一对应的关系。 我们只需重新索引下标(k, l)，使k=i+a, l=i+b、，由此可得V[i, j, a,b] = W[i, j, i+a, j+b]。 索引a和b通过在正偏移和负偏移之间移动覆盖了整个图像。 对于隐藏表示中任意给定位置（i,j）处的像素值H[i, j]，可以通过在x中以(i, j)为中心对像素进行加权求和得到，加权使用的权重为V[i,j,a,b]。
现在引用上述的第一个原则：平移不变性。 这意味着检测对象在输入X中的平移，应该仅导致隐藏表示H中的平移。也就是说，V和U实际上不依赖于(i, j)的值，即$[\mathsf{V}]_{i, j, a, b} = [\mathbf{V}]_{a, b}$。并且U是一个常数，比如。因此，我们可以简化H定义为：
$$[\mathbf{H}]_{i, j} = u + \sum_a\sum_b [\mathbf{V}]_{a, b} [\mathbf{X}]_{i+a, j+b}.$$
这就是**卷积（convolution）**。

## 局部性
如上所述，为了收集用来训练参数H[i, j]的相关信息，我们不应偏离到距(i, j)很远的地方。这意味着在$|a|> \Delta$或$|b|> \Delta$的范围之外，我们可以设置V[a, b]=0。因此，我们可以将H[i. j]重写为
$$[\mathbf{H}]_{i, j} = u + \sum_{a = -\Delta}^{\Delta} \sum_{b = -\Delta}^{\Delta} [\mathbf{V}]_{a, b}  [\mathbf{X}]_{i+a, j+b}.$$
简而言之， 这是一个**卷积层（convolutional layer）**，而卷积神经网络是包含卷积层的一类特殊的神经网络。 在深度学习研究社区中，**V被称为卷积核（convolution kernel）或者滤波器（filter）**，亦或简单地称之为该卷积层的权重，通常该权重是可学习的参数。 当图像处理的局部区域很小时，卷积神经网络与多层感知机的训练差异可能是巨大的：以前，多层感知机可能需要数十亿个参数来表示网络中的一层，而现在卷积神经网络通常只需要几百个参数，而且不需要改变输入或隐藏表示的维数。

## 小结
* 图像的平移不变性使我们以相同的方式处理局部图像，而不在乎它的位置。
* 局部性意味着计算相应的隐藏表示只需一小部分局部图像像素。
* 在图像处理中，卷积层通常比全连接层需要更少的参数，但依旧获得高效用的模型。
* 卷积神经网络（CNN）是一类特殊的神经网络，它可以包含多个卷积层。v
* 多个输入和输出通道使模型在每个空间位置可以获取图像的多方面特征。

# 图像卷积
## 互相关运算
首先，我们暂时忽略通道（第三维）这一情况，看看如何处理二维图像数据和隐藏表示,输入是高度为3、宽度为3的二维张量（即形状为3x3）。卷积核的高度和宽度都是2，而卷积核窗口（或卷积窗口）的形状由内核的高度和宽度决定（即2x2）。
![](https://zh-v2.d2l.ai/_images/correlation.svg)
图6.2.1 二维互相关运算。阴影部分是第一个输出元素，以及用于计算输出的输入张量元素和核张量元素：$0\times0+1\times1+3\times2+4\times3=19$

注意，输出大小略小于输入大小。这是因为卷积核的宽度和高度大于1， 而卷积核只与图像中每个大小完全适合的位置进行互相关运算。 所以，输出大小等于输入大小$n_h \times n_w$减去卷积核$k_h \times k_w$大小，即：
$(n_h-k_h+1) \times (n_w-k_w+1).$
这是因为我们需要足够的空间在图像上“移动”卷积核。稍后，我们将看到如何通过在图像边界周围填充零来保证有足够的空间移动卷积核，从而保持输出大小不变。 接下来，我们在corr2d函数中实现如上过程，该函数接受输入张量X和卷积核张量K，并返回输出张量Y。
```py
def corr2d(X, K):  #@save
    """计算二维互相关运算"""
    pass
X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])
K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])
corr2d(X, K)
```
卷积层对输入和卷积核权重进行互相关运算，并在添加标量偏置之后产生输出。 所以，卷积层中的两个被训练的参数是卷积核权重和标量偏置。 就像我们之前随机初始化全连接层一样，在训练基于卷积层的模型时，我们也随机初始化卷积核权重。

基于上面定义的corr2d函数实现二维卷积层。在__init__构造函数中，将weight和bias声明为两个模型参数。前向传播函数调用corr2d函数并添加偏置。如下是卷积层的一个简单应用：通过找到像素变化的位置，来检测图像中不同颜色的边缘。 首先，我们构造一个6x8像素的黑白图像。中间四列为黑色（0），其余像素为白色（1）。接下来，我们构造一个高度为1、宽度为2的卷积核K。当进行互相关运算时，如果水平相邻的两元素相同，则输出为零，否则输出为非零。现在，我们对参数X（输入）和K（卷积核）执行互相关运算。 如下所示，输出Y中的1代表从白色到黑色的边缘，-1代表从黑色到白色的边缘，其他情况的输出为0。

如果我们只需寻找黑白边缘，那么以上[1, -1]的边缘检测器足以。然而，当有了更复杂数值的卷积核，或者连续的卷积层时，我们不可能手动设计滤波器。那么我们是否可以学习由X生成Y的卷积核呢？

现在让我们看看是否可以通过仅查看“输入-输出”对来学习由X生成Y的卷积核。 我们先构造一个卷积层，并将其卷积核初始化为随机张量。接下来，在每次迭代中，我们比较Y与卷积层输出的平方误差，然后计算梯度来更新卷积核。为了简单起见，我们在此使用内置的二维卷积层，并忽略偏置。

## 特征映射和感受野
如在 6.1.4.1节中所述， 图6.2.1中输出的卷积层有时被称为特征映射（feature map），因为它可以被视为一个输入映射到下一层的空间维度的转换器。 在卷积神经网络中，对于某一层的任意元素x，其**感受野（receptive field）**是指在前向传播期间可能影响x计算的所有元素（来自所有先前层）。

请注意，感受野可能大于输入的实际大小。让我们用 图6.2.1为例来解释感受野： 给定2x2卷积核，阴影输出元素值19的感受野是输入阴影部分的四个元素。 假设之前输出为Y，其大小为2x2，现在我们在其后附加一个卷积层，该卷积层以Y为输入，输出单个元素z。 在这种情况下，Y上的的感受野z包括Y的所有四个元素，而输入的感受野包括最初所有九个输入元素。 因此，当一个特征图中的任意元素需要检测更广区域的输入特征时，我们可以构建一个**更深**的网络。
## 小结
6.2.7. 小结
二维卷积层的核心计算是二维互相关运算。最简单的形式是，对二维输入数据和卷积核执行互相关操作，然后添加一个偏置。

我们可以设计一个卷积核来检测图像的边缘。

我们可以从数据中学习卷积核的参数。

学习卷积核时，无论用严格卷积运算或互相关运算，卷积层的输出不会受太大影响。

当需要检测输入特征中更广区域时，我们可以构建一个更深的卷积网络。

## 练习
1. 构建一个具有对角线边缘的图像X。
   （1） 如果将本节中举例的卷积核K应用于X，会发生什么情况？
   （2） 如果转置X会发生什么？
   （3） 如果转置K会发生什么？

2. 在我们创建的Conv2D自动求导时，有什么错误消息？
3. 如何通过改变输入张量和卷积核张量，将互相关运算表示为矩阵乘法？
4. 手工设计一些卷积核：
    （1）二阶导数的核的形式是什么？
    （2）积分的核的形式是什么？
    （3）得到d次导数的最小核的大小是多少？


## 代码参考
```py
import torch
from torch import nn
from d2l import torch as d2l

def corr2d(X, K):  #@save
    """计算二维互相关运算"""
    h, w = K.shape
    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()
    return Y
class Conv2D(nn.Module):
    def __init__(self, kernel_size):
        super().__init__()
        self.weight = nn.Parameter(torch.rand(kernel_size))
        self.bias = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        return corr2d(x, self.weight) + self.bias
X = torch.ones((6, 8))
X[:, 2:6] = 0
X
K = torch.tensor([[1.0, -1.0]])
Y = corr2d(X, K)
Y
# 构造一个二维卷积层，它具有1个输出通道和形状为（1，2）的卷积核
conv2d = nn.Conv2d(1,1, kernel_size=(1, 2), bias=False)

# 这个二维卷积层使用四维输入和输出格式（批量大小、通道、高度、宽度），
# 其中批量大小和通道数都为1
X = X.reshape((1, 1, 6, 8))
Y = Y.reshape((1, 1, 6, 7))
lr = 3e-2  # 学习率

for i in range(10):
    Y_hat = conv2d(X)
    l = (Y_hat - Y) ** 2
    conv2d.zero_grad()
    l.sum().backward()
    # 迭代卷积核
    conv2d.weight.data[:] -= lr * conv2d.weight.grad
    if (i + 1) % 2 == 0:
        print(f'epoch {i+1}, loss {l.sum():.3f}')
```
