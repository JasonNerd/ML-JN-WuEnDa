# 前向传播、反向传播和计算图
我们已经学习了如何用小批量随机梯度下降训练模型。 然而当实现该算法时，我们只考虑了通过**前向传播（forward propagation）所涉及的计算**。 在计算梯度时，我们只调用了深度学习框架提供的**反向传播函数**，而不知其所以然。梯度的自动计算（自动微分）大大简化了深度学习算法的实现。在本节中，我们将通过一些基本的数学和计算图， 深入探讨反向传播的细节。 首先，我们将重点放在带权重衰减（正则化）的单隐藏层多层感知机上。

## 前向传播
前向传播（forward propagation或forward pass） 指的是：按顺序（从输入层到输出层）计算和存储神经网络中每层的结果。
为了简单起见，我们假设输入样本是$\mathbf{x}\in \mathbb{R}^d$， 并且我们的隐藏层不包括偏置项。 这里的中间变量是：
$$\mathbf{z}= \mathbf{W}^{(1)} \mathbf{x},$$
其中$\mathbf{W}^{(1)} \in \mathbb{R}^{h \times d}$是隐藏层的权重参数$\mathbf{z}\in \mathbb{R}^h$。 将中间变量通过激活函数$\phi$后， 我们得到长度为$h$的隐藏激活向量：
$$\mathbf{h}= \phi (\mathbf{z}).$$
隐藏变量$\mathbf{h}$也是一个中间变量。 假设输出层的参数只有权重$\mathbf{W}^{(2)} \in \mathbb{R}^{q \times h}$， 我们可以得到输出层变量，它是一个长度为$q$的向量：
$$\mathbf{o}= \mathbf{W}^{(2)} \mathbf{h}.$$
假设损失函数为$l$，样本标签为$y$，我们可以计算单个数据样本的损失项，
$$L = l(\mathbf{o}, y).$$
根据$L_2$正则化的定义，给定超参数$\lambda$，正则化项为
$$s = \frac{\lambda}{2} \left(\|\mathbf{W}^{(1)}\|_F^2 + \|\mathbf{W}^{(2)}\|_F^2\right),$$
其中矩阵的Frobenius范数是将矩阵展平为向量后应用的$L_2$范数。 最后，模型在给定数据样本上的正则化损失为：
$J = L + s.$
在下面的讨论中，我们将$J$称为目标函数（objective function）。

![](https://zh-v2.d2l.ai/_images/forward.svg)


## 反向传播


## 训练神经网络
在训练神经网络时，在初始化模型参数后， 我们交替使用前向传播和反向传播，利用反向传播给出的梯度来更新模型参数。 注意，反向传播重复利用前向传播中存储的中间值，以避免重复计算。 带来的影响之一是我们需要保留中间值，直到反向传播完成。 这也是训练比单纯的预测需要更多的内存（显存）的原因之一。 此外，这些中间值的大小与网络层的数量和批量的大小大致成正比。 因此，使用更大的批量来训练更深层次的网络更容易导致内存不足（out of memory）错误。

## 小结
1. 前向传播在神经网络定义的计算图中按顺序计算和存储中间变量，它的顺序是从输入层到输出层。

2. 反向传播按相反的顺序（从输出层到输入层）计算和存储神经网络的中间变量和参数的梯度。

3. 在训练深度学习模型时，前向传播和反向传播是相互依赖的。

4. 训练比预测需要更多的内存。
## 练习
1. 假设一些标量函数的输入是矩阵。相对于的梯度维数是多少？

2. 向本节中描述的模型的隐藏层添加偏置项（不需要在正则化项中包含偏置项）。
    画出相应的计算图。
    推导正向和反向传播方程。

3. 计算本节所描述的模型，用于训练和预测的内存占用。

4. 假设你想计算二阶导数。计算图发生了什么？你预计计算需要多长时间？

5. 假设计算图对于你的GPU来说太大了。
    你能把它划分到多个GPU上吗？
    与小批量训练相比，有哪些优点和缺点？